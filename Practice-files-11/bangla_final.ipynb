{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bltk.langtools import Tokenizer\n",
    "from bltk.langtools import UgraStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from bangla_stemmer.stemmer import stemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regexp\n",
      "  Downloading regexp-0.1.tar.gz (853 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: regexp\n",
      "  Building wheel for regexp (setup.py): started\n",
      "  Building wheel for regexp (setup.py): finished with status 'done'\n",
      "  Created wheel for regexp: filename=regexp-0.1-py3-none-any.whl size=1432 sha256=76a2f7eaac76232f51a3ce294989707ffe4e23dbd6214af6d40178cb5ecf2a59\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\d8\\37\\7d\\55cbeb7fcea2a9e1e1a48de7ba53b35f5b68994d7b67db4c55\n",
      "Successfully built regexp\n",
      "Installing collected packages: regexp\n",
      "Successfully installed regexp-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('younus.csv')\n",
    "#dataset = pd.read_excel('data.xlsx')\n",
    "dataset = dataset.dropna()\n",
    "#dataset = dataset[0:5000]\n",
    "#dataset = dataset[['comment','label']]\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>বই টি আমার সংগ্রহে আছে এবং তা আমি পড়েছি। বই টি...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ফালতু বই। শুধু শুধু টাকা নষ্ট।</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>একটু পড়ে দেখতে বলসিলো, একটু পড়লাম। তারপর হড়হড় ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>এটা কোন বই? ছিঃ ধিকার জানায়। প্রশাসনের নজর দেয়...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>প্রিয় রকমারি.কম, দয়া করে বইটি আপনাদের ওয়েবসাইট...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2006</td>\n",
       "      <td>1 স্টার দেওয়াও বৃথা। মানে ভিডিও বানাইয়া আজকে ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2007</td>\n",
       "      <td>তাদের কথা শুনলে মনে হয় তারা এই মনে হয় আশে পাশে...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2008</td>\n",
       "      <td>এইরকম বই পড়ার চাইতে মুড়ি কিন্না খাওয়াও ভালো। ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2009</td>\n",
       "      <td>বোগাস বই। টাকা নষ্ট।\\n                       ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2010</td>\n",
       "      <td>বইটি ভালো না। শুধু কিনে টাকা নষ্ট করা ছাড়া আর ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2011 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             Review  Sentiment\n",
       "0              0  বই টি আমার সংগ্রহে আছে এবং তা আমি পড়েছি। বই টি...          0\n",
       "1              1                     ফালতু বই। শুধু শুধু টাকা নষ্ট।          0\n",
       "2              2  একটু পড়ে দেখতে বলসিলো, একটু পড়লাম। তারপর হড়হড় ...          0\n",
       "3              3  এটা কোন বই? ছিঃ ধিকার জানায়। প্রশাসনের নজর দেয়...          0\n",
       "4              4  প্রিয় রকমারি.কম, দয়া করে বইটি আপনাদের ওয়েবসাইট...          0\n",
       "...          ...                                                ...        ...\n",
       "2006        2006   1 স্টার দেওয়াও বৃথা। মানে ভিডিও বানাইয়া আজকে ...          0\n",
       "2007        2007  তাদের কথা শুনলে মনে হয় তারা এই মনে হয় আশে পাশে...          0\n",
       "2008        2008   এইরকম বই পড়ার চাইতে মুড়ি কিন্না খাওয়াও ভালো। ...          0\n",
       "2009        2009   বোগাস বই। টাকা নষ্ট।\\n                       ...          0\n",
       "2010        2010  বইটি ভালো না। শুধু কিনে টাকা নষ্ট করা ছাড়া আর ...          0\n",
       "\n",
       "[2011 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_stop_words =[\"অতএব\", \"অথচ\", \"অথবা\", \"অনুযায়ী\", \"অনেক\", \"অনেকে\", \"অনেকেই\", \"অন্তত\", \"অবধি\", \"অবশ্য\", \n",
    "\"আগামী\", \"আগে\", \"আগেই\", \"আছে\", \"আজ\", \"আদ্যভাগে\", \"আপনার\", \"আপনি\", \"আবার\", \"আমরা\", \n",
    "\"আমাকে\", \"আমাদের\", \"আমার\", \"আমি\", \"আর\", \"আরও\", \"ইত্যাদি\", \"ইহা\", \"উচিত\", \"উনি\", \"উপর\", \n",
    "\"উপরে\", \"এ\", \"এঁদের\", \"এঁরা\", \"এই\", \"এক\", \"একই\", \"একজন\", \"একটা\", \"একটি\", \"একবার\", \"একে\", \n",
    "\"এখন\", \"এখনও\", \"এখানে\", \"এখানেই\", \"এটা\", \"এটাই\", \"এটি\", \"এত\", \"এতটাই\", \"এতে\", \"এদের\", \"এবং\", \n",
    "\"এবার\", \"এমন\", \"এমনকি\", \"এর\", \"এরা\", \"এলো\", \"এস\", \"এসে\", \"ঐ\", \"ও\", \"ওঁদের\", \"ওঁর\", \"ওঁরা\", \n",
    "\"ওই\", \"ওকে\", \"ওখানে\", \"ওদের\", \"ওর\", \"ওরা\", \"কখনও\", \"কত\", \"কথা\", \"কবে\", \"কয়েক\", \"কয়েকটি\", \n",
    "\"করছে\", \"করছেন\", \"করতে\", \"করবে\", \"করবেন\", \"করলে\", \"করলেন\", \"করা\", \"করাই\", \"করায়\", \"করার\", \n",
    "\"করি\", \"করিতে\", \"করিয়া\", \"করিয়ে\", \"করে\", \"করেই\", \"করেছিলেন\", \"করেছে\", \"করেছেন\", \"করেন\", \"কাউকে\", \n",
    "\"কাছ\", \"কাছে\", \"কাজ\", \"কাজে\", \"কারও\", \"কারণ\", \"কি\", \"কিংবা\", \"কিছু\", \"কিছুই\", \"কিন্তু\", \"কী\", \"কে\", \n",
    "\"কেউ\", \"কেউই\", \"কেন\", \"কোন\", \"কোনও\", \"কোনো\", \"ক্ষেত্রে\", \"খুব\", \"গিয়ে\", \"গিয়েছে\", \"গুলি\", \"গেছে\", \n",
    "\"গেল\", \"গেলে\", \"গোটা\", \"চলে\", \"চান\", \"চায়\", \"চেয়ে\", \"ছাড়া\", \"ছাড়াও\", \"ছিল\", \"ছিলেন\", \"জন\", \"জনকে\", \n",
    "\"জনের\", \"জন্য\", \"জন্যে\", \"জানতে\", \"জানা\", \"জানানো\", \"জানায়\", \"জানিয়ে\", \"জানিয়েছে\", \"টি\", \"তখন\", \n",
    "\"তত\", \"তথা\", \"তবু\", \"তবে\", \"তা\", \"তাঁকে\", \"তাঁদের\", \"তাঁর\", \"তাঁরা\", \"তাঁহারা\", \"তাই\", \"তাও\", \"তাকে\", \"তাতে\", \n",
    "\"তাদের\", \"তার\", \"তারপর\", \"তারা\", \"তারই\", \"তাহলে\", \"তাহা\", \"তাহাতে\", \"তাহার\", \"তিনই\", \"তিনি\", \"তিনিও\", \n",
    "\"তুমি\", \"তুলে\", \"তেমন\", \"তো\", \"তোমার\", \"থাকবে\", \"থাকবেন\", \"থাকা\", \"থাকায়\", \"থাকে\", \"থাকেন\", \"থেকে\", \n",
    "\"থেকেই\", \"থেকেও\", \"দিকে\", \"দিতে\", \"দিয়ে\", \"দিয়েছে\", \"দিয়েছেন\", \"দিলেন\", \"দিয়ে\", \"দু\", \"দুটি\", \"দুটো\", \n",
    "\"দেওয়া\", \"দেওয়ার\", \"দেখতে\", \"দেখা\", \"দেখে\", \"দেন\", \"দেয়\", \"দেশের\", \"দ্বারা\", \"ধরা\", \"ধরে\", \n",
    "\"নাকি\", \"নাগাদ\", \"নানা\", \"নিজে\", \"নিজেই\", \"নিজেদের\", \"নিজের\", \"নিতে\", \"নিয়ে\", \"নিয়ে\", \"নেওয়া\", \n",
    "\"নেওয়ার\", \"পক্ষে\", \"পর\", \"পরে\", \"পরেই\", \"পরেও\", \"পর্যন্ত\", \"পাওয়া\", \"পারি\", \"পারে\", \"পারেন\", \"পেয়ে\", \n",
    "\"প্রতি\", \"প্রভৃতি\", \"প্রায়\", \"ফলে\", \"ফিরে\", \"ফের\", \"বছর\", \"বদলে\", \"বরং\", \"বলতে\", \"বলল\", \"বললেন\", \"বলা\", \"বলে\", \n",
    "\"বলেছেন\", \"বলেন\", \"বসে\", \"বহু\", \"বা\", \"বাদে\", \"বার\", \"বিনা\", \"বিভিন্ন\", \"বিশেষ\", \"বিষয়টি\", \"বেশ\", \"ব্যবহার\", \n",
    "\"ব্যাপারে\", \"ভাবে\", \"ভাবেই\", \"মত\", \"মতো\", \"মতোই\", \"মধ্যভাগে\", \"মধ্যে\", \"মধ্যেই\", \"মধ্যেও\", \"মনে\", \"মাত্র\", \"মাধ্যমে\", \n",
    "\"মানুষ\", \"মানুষের\", \"মোট\", \"মোটেই\", \"যখন\", \"যত\", \"যতটা\", \"যথেষ্ট\", \"যদি\", \"যদিও\", \"যা\", \"যাঁর\", \"যাঁরা\", \"যাওয়া\", \n",
    "\"যাওয়ার\", \"যাকে\", \"যাচ্ছে\", \"যাতে\", \"যাদের\", \"যান\", \"যাবে\", \"যায়\", \"যার\", \"যারা\", \"যায়\", \"যিনি\", \"যে\", \"যেখানে\", \n",
    "\"যেতে\", \"যেন\", \"যেমন\", \"রকম\", \"রয়েছে\", \"রাখা\", \"রেখে\", \"শুধু\", \"শুরু\", \"সঙ্গে\", \"সঙ্গেও\", \"সব\", \"সবার\", \"সমস্ত\", \n",
    "\"সম্প্রতি\", \"সময়\", \"সহ\", \"সহিত\", \"সাথে\", \"সুতরাং\", \"সে\", \"সেই\", \"সেখান\", \"সেখানে\", \"সেটা\", \"সেটাই\", \"সেটাও\", \"সেটি\", \n",
    "\"স্পষ্ট\", \"স্বয়ং\", \"হইতে\", \"হইবে\", \"হইয়া\", \"হওয়া\", \"হওয়ায়\", \"হওয়ার\", \"হচ্ছে\", \"হত\", \"হতে\", \"হতেই\", \"হন\", \"হবে\", \n",
    "\"হবেন\", \"হয়\", \"হয়তো\", \"হয়নি\", \"হয়ে\", \"হয়েই\", \"হয়েছিল\", \"হয়েছে\", \"হয়েছেন\", \"হল\", \"হলে\", \"হলেই\", \"হলেও\", \"হলো\", \n",
    "\"হিসাবে\", \"হিসেবে\", \"হৈলে\", \"হোক\", \"হয়\", \"হয়ে\", \"হয়েছে\", \"অর্থাৎ\", \"অন্য\", \"অনুযায়ী\", \"অর্ধভাগে\", \"এসো\", \"কয়েক\", \n",
    "\"কয়েকটি\", \"করিয়ে\", \"করিয়া\", \"করায়\", \"গিয়ে\", \"গিয়েছে\", \"ছাড়া\", \"ছাড়াও\", \"থাকায়\", \"দিয়েছে\", \"দিয়েছেন\", \"দেয়\", \"দেওয়া\", \n",
    "\"দেওয়ার\", \"নেওয়ার\", \"পাওয়া\", \"পেয়ে\", \"প্রায়\", \"বিষয়টি\", \"যাওয়া\", \"যাওয়ার\", \"রয়েছে\", \"স্বয়ং\", \"হৈতে\", \"হইয়া\", \"হয়েছিল\", \n",
    "\"হয়েছেন\", \"হয়নি\", \"হয়েই\", \"হয়তো\", \"হওয়া\", \"হওয়ার\", \"হওয়ায়\", \"জানায়\", \"জাানিয়ে\", \"জানিয়েছে\", \"চায়\", \"চেয়ে\", \"উত্তর\", \n",
    "\"এমনি\", \"কেমনে\", \"কোটি\", \"চার\", \"চালু\", \"চেষ্টা\", \"দিন\", \"দুই\", \"নতুন\", \"পাঁচ\", \"প্রথম\", \"প্রাথমিক\", \"বক্তব্য\", \"বন\", \"বেশি\", \n",
    "\"লক্ষ\", \"সাধারণ\", \"সামনে\", \"হাজার\"]\n",
    "names = ['বাংলাদেশের','ভারত','বাংলাদেশ','তাহসান','মিথিলা','সৃজিত','তাহসানের',\n",
    "        'আয়রা','আব্দুল লতিফ','আরে','তাহসিন','শাকিব','জয়','রুবেল','দিঘীকে',\n",
    "         'দিঘি','দীঘির','দিগি','জোলিল','জলিল','অনন্ত','শাকিব','খান','তাহসিরেশন',\n",
    "         'অপুকে','অপুর','সাকিব','অপু','নাহিদ','আজহারি','জলিল','জিহাদি',\n",
    "        'শ্রীজিৎ','অনন্ত ','','','','','','']\n",
    "stop_words_for_own_dataset = ['আ‌মি','ভারতে','দে‌খছি','তোর','কারন',\n",
    "                              'বুঝা','একদিন','মানে','গেলো','হুম','রাখে',\n",
    "                              'কেমন','ই','টু','তো','কেমন','ওটা',\n",
    "                              'মেয়েটাকে','মার','স্বামীর','নাম','তোমরা','দাদা',\n",
    "                              'মহিলা','টা','চাই','টাও','র','টা','এদেরকে','র্থ',\n",
    "                              'ভাই','ভাইয়া','হজুর','আপ্নে','এইটা','ু','তারে',\n",
    "                              'তুই','তাসনিম','রে','ন','তুমার','তোমারে','কইরো',\n",
    "                              'আপনাকে','নকে','ল','থা','র','লোক','ব','জয়ের',\n",
    "                              'র','ভাইয়া','ভারত','ধর্ষকের','পোশাক','ধর্ষন',\n",
    "                              'ধর্ষনের','ধর্ষন','ধর্ষক','','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'get_emoji_regexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6340\\3833611745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_emoji_regexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'get_emoji_regexp'"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "#stemmer = UgraStemmer()\n",
    "stmr = stemmer.BanglaStemmer()\n",
    "for index,value in dataset.iterrows():\n",
    "    text = str(value['Review'])\n",
    "    text = re.sub(r'https?:\\/\\/\\S+','',text) #removing hyperlinks\n",
    "    text = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=।*/+-@#$%^&()]\", \" \", text)\n",
    "    text = re.sub(\"[A-Za-z]\",\" \",text)\n",
    "    text = re.sub(\"[0-9]\",\" \",text)\n",
    "    text = re.sub(\"[০-৯]\",\" \",text)\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace('\"', '')\n",
    "    text = emoji.get_emoji_regexp().sub(r'', text)\n",
    "      \n",
    "    temp = []\n",
    "    tokens = Tokenizer().word_tokenizer(text)\n",
    "    for word in tokens:\n",
    "        if word not in bangla_stop_words:\n",
    "            if word not in stop_words_for_own_dataset:\n",
    "                if word not in names:\n",
    "                    temp.append(word)\n",
    "    \n",
    "    text = ' '.join(temp)\n",
    "    \n",
    "#     temp = []\n",
    "#     tokens = Tokenizer().word_tokenizer(text)\n",
    "#     for word in tokens:\n",
    "#         stm = stmr.stem(word)\n",
    "#         temp.append(stm)\n",
    "#     text = ' '.join(temp)\n",
    "    \n",
    "#     tokens = Tokenizer().word_tokenizer(text)\n",
    "#     stem = stemmer.stem(tokens)\n",
    "#     text = ' '.join(stem)\n",
    "    \n",
    "    text = text.strip()\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "x = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 0\n",
    "for i in y_test:\n",
    "    if i == 1:\n",
    "        o = o + 1\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multinomial naive bayes for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the input layer and the first hidden layer\n",
    "ann.add(Dense(units = 64,activation='relu',input_dim = 1716))\n",
    "#Add the second hidden layer\n",
    "ann.add(Dense(units = 64,activation='relu'))\n",
    "#Add the output layer\n",
    "ann.add(Dense(units = 1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(x_train,y_train,validation_split=.3,batch_size=100,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = ann.predict(x_test)\n",
    "yp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for element in yp:\n",
    "    if element > 0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For conv1D dimentionality should be 187X1 where 187 is number of features and 1 = 1D Dimentionality of data\n",
    "x_train = x_train.reshape(len(x_train),x_train.shape[1],1)\n",
    "x_test = x_test.reshape(len(x_test),x_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense # for fully connected layers dense will be used\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# avoid overfitting by normalizing the samples\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Filters = Units in Dense Total number of Neurons\n",
    "    # Padding = 'same' , zero-padding, Add zero pixels all around input data\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 6, activation='relu', padding = 'same', input_shape = (1716, 1))) #we pass individual values hence not 100000,187,1\n",
    "    \n",
    "    # Normalization to avoid overfitting\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Pooling \n",
    "    model.add(MaxPooling1D(pool_size=(3), strides = (2), padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(filters = 64, kernel_size = 6, activation='relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=(3), strides = (2), padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D( filters = 64, kernel_size = 6, activation='relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=(3), strides = (2), padding = 'same'))\n",
    "\n",
    "    # Flatten \n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    # input layer\n",
    "    model.add(Dense(units = 20, activation='relu'))\n",
    "    \n",
    "    # Hidden Layer\n",
    "    model.add(Dense(units = 20, activation='relu'))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(units = 1, activation='softmax'))\n",
    "\n",
    "    # loss = 'categorical_crossentropy'\n",
    "    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for one sample, i.e. one row\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 30, batch_size = 20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate ECG Test Data\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries  \n",
    "import numpy as nm  \n",
    "import matplotlib.pyplot as mtp  \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Feature Scaling  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "sc = StandardScaler()  \n",
    "x_train = sc.fit_transform(x_train)  \n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
