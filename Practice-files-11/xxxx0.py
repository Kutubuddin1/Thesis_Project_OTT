# -*- coding: utf-8 -*-
"""xxxx0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12I1k2qbRTlC3U7AH7PAekIqeAeSOWlH9
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install xlrd==1.2.0
!pip install openpyxl
#!pip install --upgrade tensorflow
#!pip install --upgrade tensorflow-gpu
!pip install xlrd==1.2.0
!pip install openpyxl
!pip install bltk
!pip install nltk
!pip install git+https://github.com/banglakit/lemmatizer.git#egg=banglakit-lemmatizer
!pip install bnlp_toolkit
import bltk
import numpy as np
import pandas as pd
import re
import _pickle as cPickle
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import re,json,nltk
from nltk.corpus import stopwords
import nltk
from pandas import read_excel
from nltk.stem import WordNetLemmatizer
import string
from textblob import Word, TextBlob
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
class color: # Text style
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

from google.colab import drive
drive.mount('/content/drive')

df_train= pd.read_csv('/content/drive/MyDrive/ start/ott text reviews.csv' )
df_train

df_train.drop_duplicates(inplace=True)

df_train.shape

sns.countplot(df_train['Label']);

from wordcloud import WordCloud 
# Plot the Word Cloud
allWords = ' '.join([comnt for comnt in df_train['Text']])
wordCloud = WordCloud(width =1000, height =800, random_state = 21, max_font_size = 119).generate(allWords)

plt.imshow(wordCloud, interpolation = "bilinear")
plt.axis('off')
plt.show()

df_train['Label'].value_counts().plot(kind='pie')
plt.show()

"""Preprocessing/Cleaning"""

def text_to_word_list(text):
    text = text.split()
    return text

def replace_strings(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\u00C0-\u017F"          #latin
                           u"\u2000-\u206F"          #generalPunctuations
                               
                           "]+", flags=re.UNICODE)
    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)
    #latin_pattern=re.compile('[A-Za-z\u00C0-\u00D6\u00D8-\u00f6\u00f8-\u00ff\s]*',)
    
    text=emoji_pattern.sub(r'', text)
    text=english_pattern.sub(r'', text)

    return text

def remove_punctuations(my_str):
    # define punctuation
    punctuations = '''````¬£|¬¢|√ë+-*/=EROero‡ß≥‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ012‚Äì34567‚Ä¢89‡•§!()-[]{};:'"‚Äú\‚Äô,<>./?@#$%^&*_~‚Äò‚Äî‡••‚Äù‚Ä∞ü§£‚öΩÔ∏è‚úåÔøΩÔø∞‡ß∑Ôø∞'''
    
    no_punct = ""
    for char in my_str:
        if char not in punctuations:
            no_punct = no_punct + char

    # display the unpunctuated string
    return no_punct



def joining(text):
    out=' '.join(text)
    return out

def preprocessing(text):
    out=remove_punctuations(replace_strings(text))
    return out

df_train['Text'] = df_train.Text.apply(lambda x: preprocessing(str(x)))

df_train

from bnlp import BasicTokenizer
btokenizer = BasicTokenizer()
def clean_text(Text):
    
    tokens = btokenizer.tokenize(Text)

"""Stopwords Removal"""

data1 =pd.read_excel('/content/drive/MyDrive/ start/stopwords_bangla.xlsx')
stop = data1['words'].tolist()

data1

def stopwordRemoval(text):    
    x=str(text)
    l=x.split()

    stm=[elem for elem in l if elem not in stop]
    
    out=' '.join(stm)
    
    return str(out)

df_train['Text'] = df_train.Text.apply(lambda x: stopwordRemoval(str(x)))

df_train

"""Stemming"""

#make sure to turn on internet on your kernel
#importing stemmer
!pip install bangla-stemmer
from bangla_stemmer.stemmer import stemmer
## stemmer function
def stem_text (x):
  stmr = stemmer.BanglaStemmer()
  words=x.split(' ')
  stm = stmr.stem(words)
  words=(' ').join(stm)
  return words

df_train['Text']=df_train['Text'].apply(stem_text)

df_train

#df_train["Text"].apply(lambda x: TextBlob(x).words).head()
#df_train['Text'] = df_train.Text.apply(lambda x: TextBlob(str(x)))

#df_train

"""lemmatizer"""

#importing lemmatizer
from banglakit import lemmatizer as lem
from banglakit.lemmatizer import BengaliLemmatizer

## lemmatizer function

def lemm_text (x):
  lemmatizer = BengaliLemmatizer()

  words=x.split(' ')
  lem = lemmatizer.lemm(words)
  words=(' ').join(lem)
  return words

df_train

"""Category Wise Data Distribution"""

print("IN TRAIN SET...")
temp1 = df_train.groupby('Label').count()['Text'].reset_index().sort_values(by='Text',ascending=False)
temp1.style.background_gradient(cmap='Purples')

"""Count of Texts in Each Category"""

from plotly import graph_objs as go
print("On Train Set....")
fig = go.Figure(go.Funnelarea(
    text =temp1.Label,
    values = temp1.Text,
    title = {"position": "top center", "text": "Funnel-Chart of Category Distribution on Train Set"}
    ))
fig.show()

## x= df.Customer_reviewText
# y= df.Class

#Dataset Summary

def data_summary(df_train):
    
    documents = []
    words = []
    u_words = []
    total_u_words = [word.strip() for t in list(df_train.Text) for word in t.strip().split()]
    class_label= [k for k,v in df_train.Label.value_counts().to_dict().items()]
  # find word list
    for label in class_label: 
        word_list = [word.strip() for t in list(df_train[df_train.Label==label].Text) for word in t.strip().split()]
        counts = dict()
        for word in word_list:
                counts[word] = counts.get(word, 0)+1
                
        # sort the dictionary of word list  
        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)
        
        # Documents per class
        documents.append(len(list(df_train[df_train.Label==label].Text)))
        
        # Total Word per class
        words.append(len(word_list))
        
        # Unique words per class 
        u_words.append(len(np.unique(word_list)))
       
        print("\nClass Name : ",label)
        print("Number of Documents:{}".format(len(list(df_train[df_train.Label==label].Text))))  
        print("Number of Words:{}".format(len(word_list))) 
        print("Number of Unique Words:{}".format(len(np.unique(word_list)))) 
        print("Most Frequent Words:\n")
        for k,v in ordered[:10]:
              print("{}\t{}".format(k,v))
                
    print("Total Number of Unique Words:{}".format(len(np.unique(total_u_words))))           
   
    return documents,words,u_words,class_label


documents,words,u_words,class_names = data_summary(df_train)

my_str = ' '

try:
    result = float(my_str)
except ValueError:
    result = 0


print(result)  # üëâÔ∏è 0

from imblearn.over_sampling import SMOTE
from collections import Counter
from matplotlib import pyplot

oversample = SMOTE()

X, y = oversample.fit_resample(X, y)

"""**Train-Test Spliting**"""

#x= df_train.Text.values[:4769]
#y= df_train.Label.values[:4769]
from sklearn.model_selection import train_test_split
X = df_train.Text
y = df_train.Label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)

X_train.shape,X_test.shape,y_train.shape,y_test.shape

"""**Applying Logestic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

lr = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', LogisticRegression()),
              ])

lr.fit(X_train,y_train)
y_pred1 = lr.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred1,y_test)))



from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred1))

"""**Applying DecisionTreeClassifier**"""

from sklearn.tree import DecisionTreeClassifier


decisiontreeclassifier = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', DecisionTreeClassifier()),
              ])
decisiontreeclassifier.fit(X_train, y_train)

y_pred = decisiontreeclassifier.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier

randomforestclassifier = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', RandomForestClassifier()),
              ])
randomforestclassifier.fit(X_train, y_train)

y_pred = randomforestclassifier.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying MultinomialNB**"""

from sklearn.naive_bayes import MultinomialNB


naivebayes = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])
naivebayes.fit(X_train, y_train)

y_pred = naivebayes.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying KNeighborsClassifier**"""

from sklearn.neighbors import KNeighborsClassifier

KNeighborsClassifier = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', KNeighborsClassifier()),
              ])
KNeighborsClassifier.fit(X_train, y_train)

y_pred = KNeighborsClassifier.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying SVC**"""

from sklearn.svm import SVC

SVC = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', SVC()),
              ])
SVC.fit(X_train, y_train)

y_pred = SVC.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying SGDClassifier**"""

from sklearn.linear_model import SGDClassifier

SGDClassifier = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', SGDClassifier()),
              ])
SGDClassifier.fit(X_train, y_train)

y_pred = SGDClassifier.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**Applying XGBClassifier**"""

from xgboost import XGBClassifier

xgboost = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', XGBClassifier()),
              ])
xgboost.fit(X_train, y_train)

y_pred = xgboost.predict(X_test)

print("Accuracy: {0:.2%}".format(accuracy_score(y_pred,y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test,y_pred))

"""**split and build model another way..**"""

from sklearn.feature_extraction.text import CountVectorizer

# Create a Vectorizer Object
vectorizer = CountVectorizer()
 
vectorizer.fit(x)
 
# Printing the identified Unique words along with their indices
# print("Vocabulary: ", vectorizer.vocabulary_)
 
# Encode the Document
vector = vectorizer.transform(x)
 
# Summarizing the Encoded Texts
# print("Encoded Document is:")
print(vector.toarray())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(vector, y, test_size=0.2, shuffle=True, random_state=100)

# X_train_counts.toarray()

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier

model = LogisticRegression()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model = RandomForestClassifier()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model = MultinomialNB()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model = KNeighborsClassifier()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model =SVC()
model.fit(X_train, y_train)

model.score( X_test,y_test)

model =SGDClassifier()
model.fit(X_train, y_train)

model.score( X_test,y_test)